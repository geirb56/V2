"""
CardioCoach - Module LLM Emergent (GPT-4o-mini)
# LLM serveur uniquement ‚Äì pas d'ex√©cution client-side

Ce module g√®re l'int√©gration avec Emergent Universal LLM Key pour g√©n√©rer 
des r√©ponses naturelles et conversationnelles via GPT-4o-mini.

Fallback automatique: si l'appel LLM √©choue (cr√©dits √©puis√©s, timeout, erreur),
le syst√®me revient aux templates Python rule-based.
"""

import os
import time
import logging
from typing import Dict, List, Optional, Tuple
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

# Configuration Emergent LLM
EMERGENT_LLM_KEY = os.environ.get("EMERGENT_LLM_KEY", "")
LLM_MODEL = "gpt-4.1-mini"  # GPT-4o-mini equivalent
LLM_PROVIDER = "openai"
LLM_TIMEOUT = 15  # secondes max pour une r√©ponse

# Prompt syst√®me pour le coach (identique √† Ollama)
SYSTEM_PROMPT = """Tu es CardioCoach, un coach running enthousiaste, motivant, positif et expert en plans d'entra√Ænement.

PERSONNALIT√â:
- Tu parles naturellement en fran√ßais courant, comme un ami coach
- Tu utilises des √©motic√¥nes avec parcimonie (1-2 max par message) üèÉüí™
- Tu poses parfois des questions ouvertes pour continuer la conversation
- Tu es bienveillant, encourageant et fun - jamais de jugement n√©gatif
- Tu gardes tes r√©ponses concises (3-5 phrases max)

R√àGLES IMPORTANTES:
- Base tes conseils UNIQUEMENT sur les donn√©es Strava fournies ci-dessous
- Ne fabule pas, si tu n'as pas l'info, dis-le
- Encourage toujours, m√™me si la performance n'est pas top
- Utilise le tutoiement
- R√©ponds directement √† la question pos√©e

DONN√âES UTILISATEUR (Strava):
{context_data}

HISTORIQUE DE LA CONVERSATION:
{conversation_history}
"""


async def check_llm_available() -> bool:
    """V√©rifie si la cl√© Emergent LLM est configur√©e"""
    return bool(EMERGENT_LLM_KEY) and EMERGENT_LLM_KEY.startswith("sk-emergent")


async def generate_llm_response(
    user_message: str,
    context: Dict,
    conversation_history: List[Dict],
    user_id: str = "unknown"
) -> Tuple[Optional[str], bool, dict]:
    """
    G√©n√®re une r√©ponse via Emergent LLM (GPT-4o-mini).
    
    # LLM serveur uniquement ‚Äì pas d'ex√©cution client-side
    
    Args:
        user_message: Question de l'utilisateur
        context: Donn√©es RAG (workouts, stats, etc.)
        conversation_history: Historique des √©changes r√©cents
        user_id: ID utilisateur pour les logs
        
    Returns:
        Tuple[response_text, success_flag, metadata]
        - Si success=True: response contient la r√©ponse LLM
        - Si success=False: response est None, utiliser le fallback templates
        - metadata: infos sur le temps de g√©n√©ration, tokens, etc.
    """
    start_time = time.time()
    metadata = {
        "model": LLM_MODEL,
        "provider": LLM_PROVIDER,
        "duration_sec": 0,
        "success": False
    }
    
    # V√©rifier si la cl√© est disponible
    if not await check_llm_available():
        logger.warning(f"[LLM] Emergent LLM Key non configur√©e")
        return None, False, metadata
    
    # Construire le contexte utilisateur pour le prompt
    context_data = _build_context_string(context)
    history_str = _build_history_string(conversation_history)
    
    # Construire le prompt syst√®me complet
    system = SYSTEM_PROMPT.format(
        context_data=context_data,
        conversation_history=history_str
    )
    
    try:
        # Import Emergent LLM
        from emergentintegrations.llm.chat import LlmChat, UserMessage
        
        # Cr√©er une session unique pour cet utilisateur
        session_id = f"cardiocoach_{user_id}_{int(time.time())}"
        
        # Initialiser le chat avec GPT-4o-mini
        chat = LlmChat(
            api_key=EMERGENT_LLM_KEY,
            session_id=session_id,
            system_message=system
        ).with_model(LLM_PROVIDER, LLM_MODEL)
        
        # Cr√©er le message utilisateur
        user_msg = UserMessage(text=user_message)
        
        # Envoyer et obtenir la r√©ponse
        import asyncio
        response = await asyncio.wait_for(
            chat.send_message(user_msg),
            timeout=LLM_TIMEOUT
        )
        
        elapsed = time.time() - start_time
        metadata["duration_sec"] = round(elapsed, 2)
        metadata["success"] = True
        
        # Nettoyer la r√©ponse
        llm_response = _clean_response(str(response))
        
        if llm_response:
            logger.info(f"[LLM] ‚úÖ R√©ponse g√©n√©r√©e par {LLM_MODEL} en {elapsed:.2f}s pour user {user_id}")
            return llm_response, True, metadata
        else:
            logger.warning("[LLM] R√©ponse vide du mod√®le")
            return None, False, metadata
            
    except asyncio.TimeoutError:
        elapsed = time.time() - start_time
        metadata["duration_sec"] = round(elapsed, 2)
        logger.warning(f"[LLM] ‚è±Ô∏è Timeout apr√®s {elapsed:.2f}s pour user {user_id}")
        return None, False, metadata
        
    except Exception as e:
        elapsed = time.time() - start_time
        metadata["duration_sec"] = round(elapsed, 2)
        error_msg = str(e)
        
        # V√©rifier si c'est un probl√®me de cr√©dits
        if "credit" in error_msg.lower() or "balance" in error_msg.lower():
            logger.error(f"[LLM] üí≥ Cr√©dits insuffisants pour user {user_id}")
        else:
            logger.error(f"[LLM] ‚ùå Erreur: {error_msg}")
        
        return None, False, metadata


def _build_context_string(context: Dict) -> str:
    """Construit une description humanis√©e du contexte utilisateur pour le RAG"""
    parts = []
    
    # Stats de la semaine
    km_semaine = context.get("km_semaine", 0)
    nb_seances = context.get("nb_seances", 0)
    allure = context.get("allure", "N/A")
    cadence = context.get("cadence", 0)
    
    if km_semaine > 0:
        parts.append(f"‚Ä¢ Cette semaine: {km_semaine} km en {nb_seances} s√©ance(s)")
    if allure != "N/A":
        parts.append(f"‚Ä¢ Allure moyenne r√©cente: {allure}/km")
    if cadence > 0:
        parts.append(f"‚Ä¢ Cadence moyenne: {cadence} spm")
    
    # Zones cardiaques
    zones = context.get("zones", {})
    if zones:
        z1z2 = zones.get("z1", 0) + zones.get("z2", 0)
        z3 = zones.get("z3", 0)
        z4z5 = zones.get("z4", 0) + zones.get("z5", 0)
        parts.append(f"‚Ä¢ R√©partition zones: {z1z2}% endurance (Z1-Z2), {z3}% tempo (Z3), {z4z5}% intensit√© (Z4-Z5)")
    
    # Derni√®res s√©ances
    recent = context.get("recent_workouts", [])
    if recent:
        parts.append("‚Ä¢ Derni√®res sorties:")
        for w in recent[:3]:
            name = w.get('name', 'Run')
            dist = w.get('distance_km', 0)
            dur = w.get('duration_min', 0)
            if dist > 0:
                parts.append(f"  - {name}: {dist} km en {dur} min")
    
    # Ratio charge/r√©cup
    ratio = context.get("ratio", 1.0)
    if ratio > 1.3:
        parts.append("‚Ä¢ ‚ö†Ô∏è Charge √©lev√©e cette semaine vs la pr√©c√©dente")
    elif ratio < 0.8:
        parts.append("‚Ä¢ Charge l√©g√®re cette semaine, marge pour augmenter")
    else:
        parts.append("‚Ä¢ Charge √©quilibr√©e cette semaine")
    
    # Objectif course
    if context.get("objectif_nom"):
        jours = context.get("jours_course", "?")
        parts.append(f"‚Ä¢ Objectif: {context['objectif_nom']} dans {jours} jours")
    
    # Split analysis si disponible
    if context.get("split_analysis"):
        sa = context["split_analysis"]
        if sa.get("fastest_km"):
            parts.append(f"‚Ä¢ Derni√®re s√©ance - Km le + rapide: Km{sa['fastest_km']}, Km le + lent: Km{sa.get('slowest_km', '?')}")
    
    return "\n".join(parts) if parts else "Pas encore de donn√©es d'entra√Ænement disponibles."


def _build_history_string(history: List[Dict]) -> str:
    """Construit l'historique de conversation pour le contexte LLM"""
    if not history:
        return "D√©but de conversation"
    
    # Garder les 4-5 derniers √©changes max
    recent_history = history[-5:]
    lines = []
    
    for msg in recent_history:
        role = "Utilisateur" if msg.get("role") == "user" else "Coach"
        content = msg.get("content", "")[:200]  # Tronquer si trop long
        lines.append(f"{role}: {content}")
    
    return "\n".join(lines)


def _clean_response(response: str) -> str:
    """Nettoie la r√©ponse LLM"""
    if not response:
        return ""
    
    response = response.strip()
    
    # Supprimer les guillemets en d√©but/fin si pr√©sents
    if response.startswith('"') and response.endswith('"'):
        response = response[1:-1]
    
    # Limiter la longueur raisonnable
    if len(response) > 600:
        # Couper au dernier point ou emoji
        response = response[:600]
        last_period = max(response.rfind("."), response.rfind("!"), response.rfind("?"))
        if last_period > 300:
            response = response[:last_period + 1]
    
    return response.strip()


# Fonction pour obtenir les infos du mod√®le utilis√©
def get_llm_info() -> dict:
    """Retourne les informations sur le mod√®le LLM configur√©"""
    return {
        "provider": LLM_PROVIDER,
        "model": LLM_MODEL,
        "key_configured": bool(EMERGENT_LLM_KEY),
        "timeout_sec": LLM_TIMEOUT
    }


# Export pour utilisation dans server.py
__all__ = [
    "generate_llm_response", 
    "check_llm_available", 
    "get_llm_info",
    "LLM_MODEL", 
    "LLM_PROVIDER"
]
